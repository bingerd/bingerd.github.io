<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Serving PyTorch Models with Rust — Bing Tan</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="description" content="Senior-level guide on using Rust for serving PyTorch models efficiently in production." />

  <style>
    :root {
      --bg: #000; --fg: #fff; --glass: rgba(0,0,0,0.45); --accent: #d9a4ff;
    }
    [data-theme="light"] {
      --bg: #f6f6f6; --fg: #0b0b0b; --glass: rgba(255,255,255,0.7); --accent: #7a3cff;
    }

    body { margin:0; font-family: system-ui, -apple-system, BlinkMacSystemFont, sans-serif; background:var(--bg); color:var(--fg); line-height:1.7; }
    nav { position:fixed; top:0; width:100%; display:flex; justify-content:space-between; padding:14px 24px; backdrop-filter:blur(12px); background:var(--glass); z-index:10; }
    nav .logo{ font-weight:700; letter-spacing:0.12em; }
    nav ul{ display:flex; gap:16px; list-style:none; margin:0; padding:0; align-items:center; }
    nav li{ cursor:pointer; opacity:0.8; }
    nav li:hover{ opacity:1; }
    nav a{ color:inherit; text-decoration:none; }
    main{ max-width:860px; margin:140px auto 80px; padding:0 24px; }
    h1{ font-size:clamp(2.4rem,5vw,3.8rem); margin-bottom:0.8rem; }
    h2{ font-size:clamp(1.6rem,3vw,2.2rem); margin-top:2.4rem; color:var(--accent); }
    p{ margin-bottom:1.5rem; }
    code{ background: rgba(255,255,255,0.08); padding:2px 6px; border-radius:6px; }
    pre{ background: rgba(255,255,255,0.08); padding:16px; border-radius:12px; overflow-x:auto; }
    blockquote{ border-left:4px solid var(--accent); padding-left:12px; opacity:0.8; margin:1.5rem 0; }
    footer{ text-align:center; padding:48px 24px; opacity:0.6; }
    button.toggle{ background:none; border:1px solid currentColor; padding:6px 12px; border-radius:999px; cursor:pointer; }
  </style>
</head>
<body data-theme="dark">

<nav>
  <div class="logo">BING TAN</div>
  <ul>
    <li><a href="../../index.html">Home</a></li>
    <li><a href="../../docs/blog.html">Writing</a></li>
    <li><button class="toggle" onclick="toggleTheme()">Theme</button></li>
  </ul>
</nav>

<main>
  <h1>Serving PyTorch Models with Rust</h1>
  <p>Rust is increasingly popular for production ML serving due to its performance, memory safety, and concurrency model. In this article, we explore how to serve PyTorch models efficiently using Rust and related tooling.</p>

  <h2>Why Rust for ML Serving?</h2>
  <ul>
    <li>Low-latency, memory-safe concurrent servers</li>
    <li>Seamless integration with Python through FFI or TorchScript</li>
    <li>Excellent performance for high-throughput inference</li>
  </ul>

  <h2>Workflow Overview</h2>
  <p>A typical Rust-based PyTorch serving pipeline:</p>
  <ol>
    <li>Export the model using <code>torch.jit.trace</code> or <code>torch.jit.script</code>.</li>
    <li>Load the TorchScript model in Rust via <code>tch-rs</code> crate.</li>
    <li>Expose inference through <code>axum</code> or <code>warp</code> HTTP server.</li>
    <li>Implement batching and async handling for high throughput.</li>
  </ol>

  <h2>Example: Loading TorchScript in Rust</h2>
  <pre><code>use tch::CModule;

fn main() -> anyhow::Result<()> {
    let model = CModule::load("model.pt")?;
    let input = tch::Tensor::of_slice(&[1.0, 2.0, 3.0]).unsqueeze(0);
    let output = model.forward_ts(&[input])?;
    println!("{:?}", output);
    Ok(())
}</code></pre>

  <h2>Serving with Axum</h2>
  <p>Combine Rust's async server with TorchScript inference:</p>
  <pre><code>use axum::{routing::post, Router};
use std::sync::Arc;
use tch::CModule;

#[tokio::main]
async fn main() {
    let model = Arc::new(CModule::load("model.pt").unwrap());
    let app = Router::new()
        .route("/predict", post(move |body| {
            // deserialize input and run inference
        }));
    axum::Server::bind(&"0.0.0.0:8080".parse().unwrap())
        .serve(app.into_make_service())
        .await
        .unwrap();
}</code></pre>

  <h2>Key Tips</h2>
  <ul>
    <li>Use TorchScript for full Rust integration, avoiding Python runtime.</li>
    <li>Consider <code>tokio::spawn</code> for async batching.</li>
    <li>Benchmark latency and throughput — Rust servers can easily reach sub-ms per request for small models.</li>
  </ul>

  <h2>Conclusion</h2>
  <p>Rust can provide a powerful, safe, and high-performance backend for PyTorch inference, ideal for production environments with stringent latency requirements.</p>
</main>

<footer>© 2026 Bing Tan</footer>
<script>
  window.toggleTheme = () => { document.body.dataset.theme = document.body.dataset.theme === 'dark' ? 'light':'dark'; }
</script>
</body>
</html>
