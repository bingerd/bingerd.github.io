<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Transformers, KV Caching, and Scaling Large Language Models — Bing Tan</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="description" content="Deep dive into transformer architectures, KV caching, and strategies for scaling large language models efficiently." />

  <style>
    :root { --bg:#000; --fg:#fff; --glass:rgba(0,0,0,0.45); --accent:#d9a4ff; }
    [data-theme="light"] { --bg:#f6f6f6; --fg:#0b0b0b; --glass:rgba(255,255,255,0.7); --accent:#7a3cff; }

    body { margin:0; font-family:system-ui,-apple-system,BlinkMacSystemFont,sans-serif; background:var(--bg); color:var(--fg); line-height:1.7; }
    nav { position:fixed; top:0; width:100%; display:flex; justify-content:space-between; padding:14px 24px; backdrop-filter:blur(12px); background:var(--glass); z-index:10; }
    nav .logo{ font-weight:700; letter-spacing:0.12em; }
    nav ul{ display:flex; gap:16px; list-style:none; margin:0; padding:0; align-items:center; }
    nav li{ cursor:pointer; opacity:0.8; }
    nav li:hover{ opacity:1; }
    nav a{ color:inherit; text-decoration:none; }

    main{ max-width:860px; margin:140px auto 80px; padding:0 24px; }
    h1{ font-size:clamp(2.4rem,5vw,3.8rem); margin-bottom:0.8rem; }
    h2{ font-size:clamp(1.6rem,3vw,2.2rem); margin-top:2.4rem; color:var(--accent); }
    h3{ margin-top:1.8rem; color:var(--accent); }
    p{ margin-bottom:1.5rem; }
    code{ background: rgba(255,255,255,0.08); padding:2px 6px; border-radius:6px; }
    pre{ background: rgba(255,255,255,0.08); padding:16px; border-radius:12px; overflow-x:auto; }
    blockquote{ border-left:4px solid var(--accent); padding-left:12px; opacity:0.8; margin:1.5rem 0; }
    footer{ text-align:center; padding:48px 24px; opacity:0.6; }
    button.toggle{ background:none; border:1px solid currentColor; padding:6px 12px; border-radius:999px; cursor:pointer; }
  </style>
</head>
<body data-theme="dark">

<nav>
  <div class="logo">BING TAN</div>
  <ul>
    <li><a href="../../index.html">Home</a></li>
    <li><a href="../../docs/blog.html">Writing</a></li>
    <li><button class="toggle" onclick="toggleTheme()">Theme</button></li>
  </ul>
</nav>

<main>
  <h1>Transformers, KV Caching, and Scaling Large Language Models</h1>
  <p>Large transformer models like GPT and LLaMA require careful engineering to serve efficiently. This article explores transformer internals, KV caching, and production optimizations for scaling inference.</p>

  <h2>1. Transformer Architecture Recap</h2>
  <p>Transformers use self-attention mechanisms to capture dependencies across sequences. Each layer has:</p>
  <ul>
    <li>Multi-head attention</li>
    <li>Feed-forward networks</li>
    <li>Layer normalization and residual connections</li>
  </ul>
  <p>The attention mechanism computes:</p>
  <pre><code>Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V</code></pre>

  <h2>2. Autoregressive Decoding Challenges</h2>
  <p>Generating tokens sequentially is expensive because naive decoding recomputes all past attention keys and values at every step.</p>

  <h2>3. KV Caching for Efficiency</h2>
  <p>Key-Value (KV) caching stores the projected <code>K</code> and <code>V</code> tensors for each past token:</p>
  <ul>
    <li>Reduces computation from O(n²) → O(n)</li>
    <li>Enables faster streaming generation</li>
  </ul>
  <p>In practice, during inference:</p>
  <pre><code>cache = []
for t, x in enumerate(tokens):
    k, v = compute_kv(x)
    cache.append((k,v))
    y = attention(query=x, keys=stack(cache.k), values=stack(cache.v))</code></pre>

  <h2>4. Batch and Memory Optimizations</h2>
  <ul>
    <li>Use FP16/BF16 for inference</li>
    <li>Implement rotary embeddings efficiently</li>
    <li>Streamline GPU memory with chunked attention</li>
  </ul>

  <h2>5. Serving at Scale</h2>
  <p>Key considerations:</p>
  <ul>
    <li>Shard KV caches across GPUs for multi-GPU serving</li>
    <li>Use async pipelines to keep GPUs saturated</li>
    <li>Monitor latency per token and batch requests</li>
  </ul>

  <h2>6. Practical Insights</h2>
  <blockquote>
    “KV caching transforms transformer inference from brute-force recomputation to a scalable, production-ready operation.” — ML Engineering Best Practices
  </blockquote>
  <p>Combining batching, sharding, and KV caching enables near real-time inference for very large language models.</p>

  <h2>7. Conclusion</h2>
  <p>Transformers are powerful, but serving them efficiently requires deep understanding of internals and careful engineering. KV caching, batching, and memory optimization are essential for production-grade LLM systems.</p>
</main>

<footer>© 2026 Bing Tan</footer>

<script>
  window.toggleTheme = () => { document.body.dataset.theme = document.body.dataset.theme === 'dark' ? 'light':'dark'; }
</script>
</body>
</html>
